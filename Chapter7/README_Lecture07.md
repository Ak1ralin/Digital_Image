# Lecture 07: Image Compression
---

## üìå Objectives
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à **data, information, redundancy** ‡πÉ‡∏ô‡∏†‡∏≤‡∏û  
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ **lossless ‡πÅ‡∏•‡∏∞ lossy compression**  
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô **JPEG**„Äê95‚Ä†source„Äë  

---

## üìö Core Concepts

### 1. Fundamentals
- Image compression = ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå ‡πÇ‡∏î‡∏¢‡∏¢‡∏±‡∏á retain ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô  
- Compression ratio:  

   $ C_R = \frac{ori}{compress} $ - can be any information-carrying unit  
- Relative data redundancy

   $ R_D = 1 - \frac{1}{C_R} $

- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:  
  - **Lossless compression** ‚Üí exact reconstruction, ratio 2‚Äì10, 100% reconstruct, 
      - medical & business, satellite  
  - **Lossy compression** ‚Üí ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏π‡∏ç‡∏´‡∏≤‡∏¢‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô, ratio 10‚Äì50, ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

- Some definition :
   - uncompressed (original) --compress--> compressed --reconstruct--> decompressed 
   - Data : raw data, might redundancy (give same info) so we reduce it
   - Information : interpretation of data in meaningful way
- Redundancy types : root cause
  1. Coding redundancy : represent data in a wrong way 
      - Symbol Encoder : ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ó‡∏≥‡πÉ‡∏´‡πâ code optimal = ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ entropy 
      - $L_{avg} = \sum_{k=0}^{L-1} l(r_k)p_r(r_k)$
         - ‡πÉ‡∏ä‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ coding ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏µ‡πà‡∏ö‡∏¥‡∏ó‡∏ï‡πà‡∏≠ pixel, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ entropy ‡∏Ñ‡∏∑‡∏≠‡∏î‡∏µ ‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ
         - $l(r_k)$ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏¥‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á $r_k$
         - $p_r(r_k)$ ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ó‡∏µ‡πà $r_k$ ‡∏à‡∏∞‡∏õ‡∏£‡∏≤‡∏Å‡∏é
  2. Interpixel redundancy : adjacent pixel tend to be highly correlated
      - Mapper : ‡πÉ‡∏ä‡πâ different ‡πÅ‡∏ó‡∏ô‡∏ö‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤‡∏ï‡∏£‡∏á‡πÜ
  3. Psychovisual redundancy : fully detailed in unnessary part  
      - Quantizer : ‡∏•‡∏î resolution => lossy ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏∑‡∏≠ lossy compression
---

### 2. Elements of Information Theory
- Information of event E:  

$ I(E) = -\log P(E) $  

- Entropy/uncertainty of source:  

$ H(z) = - \sum p(z_k) \log_2 p(z_k) $  
   - log2 ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏õ‡πá‡∏ô bit

- Entropy ‚âà minimum average bits/pixel needed

---

### 3. Image Compression Models
General model:  
Source encoder ‚Üí Channel encoder ‚Üí Transmission ‚Üí Channel decoder ‚Üí Source
- Components:  
  - Mapper  
  - Quantizer : for lossy compression only
  - Symbol encoder  
  - Inverse mapper

---

### 4. Error-Free (Lossless) Compression
1. **Variable-Length Coding**  
   - Huffman coding: ‡∏™‡∏£‡πâ‡∏≤‡∏á tree ‡∏à‡∏≤‡∏Å histogram probability
      - ‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏∏‡∏î‡∏™‡∏≠‡∏á‡∏≠‡∏±‡∏ô‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡∏à‡∏ô‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà 2 
      - ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° 0 ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏ï‡∏¥‡∏° 1   
   - Arithmetic coding: encode ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç real [0,1), ‡πÉ‡∏ä‡πâ distribution  

2. **LZW Coding**  
   - ‡πÉ‡∏ä‡πâ dictionary-based compression, ‡πÉ‡∏ä‡πâ 9 bits 256 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö pattern ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô dict  
   - ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô GIF, TIFF, PDF

3. **Bit-plane Coding**  
   - ‡πÅ‡∏¢‡∏Å plane ‡∏Ç‡∏≠‡∏á bits MSB = ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏∏‡∏î, ‡πÉ‡∏ä‡πâ run-length coding  

4. **Run-Length Coding (RLC)**  
   - ‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏Å‡∏±‡∏ö binary images, encode (value, length)
   - ‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πâ‡∏≠‡∏ô‡πÜ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pixel ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
   - binary horizontal : ‡∏°‡∏µ‡πÅ‡∏Ñ‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡πá‡∏û‡∏≠ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ 0
   - extended : (color, #run) : ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô tuple pair

5. **Lossless Predictive Coding**  
   - ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≤‡∏î‡πÄ‡∏î‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô ‚Üí encode error  

---

### 5. Lossy Compression
1. **Lossy Predictive Coding**  
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° quantizer ‚Üí ‡∏ö‡∏µ‡∏ö error ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏à‡∏≥‡∏Å‡∏±‡∏î  

2. **Transform Coding**  
   - ‡πÉ‡∏ä‡πâ DCT (Discrete Cosine Transform) ‚Üí JPEG baseline  
      - ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ real number ‡πÄ‡∏•‡∏¢‡πÄ‡∏õ‡πá‡∏ô lossy
   - ‡πÉ‡∏ä‡πâ Wavelet ‚Üí JPEG2000  

**JPEG Steps:**  
- Level shift pixels (‚àí128)  
- DCT (8√ó8 block)  
- Quantization (lossy step)  
- Zig-zag scan (DC, AC coefficients)  
- Entropy coding (Huffman/Arithmetic)  

---

### 6. Standards Summary
- **JPEG**: DCT, quantization, Huffman, 8√ó8 blocks  
- **JPEG2000**: wavelet, lossless+lossy, arithmetic coding  
- **PNG**: DEFLATE (Huffman + LZ77)  
- **BMP/TIFF**: LZW, RLC  
- **Predictive Coding**: DPCM (multimedia bandwidth reduction)  

---

## üêç Python Example (Huffman via `heapq`)
```python
import heapq
from collections import Counter, namedtuple

class Node(namedtuple("Node", ["left", "right"])):
    def walk(self, code, acc):
        self.left.walk(code, acc + "0")
        self.right.walk(code, acc + "1")

class Leaf(namedtuple("Leaf", ["char"])):
    def walk(self, code, acc):
        code[self.char] = acc or "0"

def huffman_encode(s):
    h = []
    for ch, freq in Counter(s).items():
        h.append((freq, len(h), Leaf(ch)))
    heapq.heapify(h)
    count = len(h)
    while len(h) > 1:
        freq1, _c1, left = heapq.heappop(h)
        freq2, _c2, right = heapq.heappop(h)
        heapq.heappush(h, (freq1 + freq2, count, Node(left, right)))
        count += 1
    code = {}
    if h:
        [(_freq, _count, root)] = h
        root.walk(code, "")
    return code

# Example usage
s = "ABBCCCDDDDEEEEE"
code = huffman_encode(s)
print(code)
```

---

## ‚úÖ Summary
- Compression = ‡∏•‡∏î redundancy ‚Üí save storage, bandwidth  
- Lossless (Huffman, Arithmetic, LZW, RLC, Predictive)  
- Lossy (Transform coding, Quantization, Predictive)  
- JPEG = DCT + quantization + Huffman + zig-zag coding  
- JPEG2000 = wavelet + arithmetic coding  
- Standards = JPEG, JPEG2000, PNG, TIFF, BMP„Äê95‚Ä†source„Äë  
